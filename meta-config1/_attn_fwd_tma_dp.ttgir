#blocked = #ttg.blocked<{sizePerThread = [1, 128], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 64], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#loc = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0)
#loc1 = loc(unknown)
#loc7 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":457:23)
#loc14 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":473:16)
#loc15 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":475:21)
#loc17 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":476:21)
#loc18 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":174:23)
#loc19 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":210:116)
#loc20 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":484:44)
#loc21 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":209:116)
#loc22 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":206:24)
#loc23 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":203:78)
#loc24 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":148:19)
#loc25 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":207:24)
#loc26 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":155:31)
#loc38 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":155:42)
#loc44 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":160:21)
#shared = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = false, elementBitWidth = 16}>
#shared1 = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0]}>
#shared2 = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = true, elementBitWidth = 16}>
#smem = #ttg.shared_memory
#tmem = #ttng.tensor_memory_encoding<blockM = 128, blockN = 128, unpacked = true>
#tmem1 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 64, unpacked = true>
#tmem2 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 128, unpacked = false>
#loc68 = loc(callsite(#loc19 at #loc20))
#loc69 = loc(callsite(#loc21 at #loc20))
#loc70 = loc(callsite(#loc22 at #loc20))
#loc71 = loc(callsite(#loc23 at #loc20))
#loc72 = loc(callsite(#loc25 at #loc20))
#loc76 = loc(callsite(#loc18 at #loc68))
#loc77 = loc(callsite(#loc18 at #loc69))
#loc78 = loc(callsite(#loc24 at #loc69))
#loc79 = loc(callsite(#loc24 at #loc68))
#loc80 = loc(callsite(#loc26 at #loc69))
#loc81 = loc(callsite(#loc26 at #loc68))
#loc94 = loc(callsite(#loc38 at #loc69))
#loc98 = loc(callsite(#loc44 at #loc69))
#loc102 = loc(callsite(#loc38 at #loc68))
#loc108 = loc(callsite(#loc44 at #loc68))
#loc113 = loc(callsite(#loc1 at #loc94))
#loc115 = loc(callsite(#loc1 at #loc98))
#loc117 = loc(callsite(#loc1 at #loc102))
#loc119 = loc(callsite(#loc1 at #loc108))
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:100", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_attn_fwd_tma_dp(%arg0: f32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg2: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg4: !tt.tensordesc<tensor<128x128xf16, #shared>> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg5: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg6: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg7: i64 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg8: i64 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg9: !tt.tensordesc<tensor<128x128xf16, #shared>> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg10: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg11: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg12: i64 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg13: i64 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg14: !tt.tensordesc<tensor<128x128xf16, #shared>> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg15: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg16: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg17: i64 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg18: i64 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg19: !tt.tensordesc<tensor<128x128xf16, #shared>> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg20: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg21: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg22: i64 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg23: i64 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg24: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0)) attributes {noinline = false} {
    %c3_i32 = arith.constant 3 : i32 loc(#loc1)
    %c-1_i32 = arith.constant -1 : i32 loc(#loc1)
    %c2_i32 = arith.constant 2 : i32 loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %cst = arith.constant dense<0.000000e+00> : tensor<128x128xf32, #blocked> loc(#loc1)
    %cst_0 = arith.constant dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
    %c128_i32 = arith.constant 128 : i32 loc(#loc1)
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %cst_1 = arith.constant 1.44269502 : f32 loc(#loc1)
    %c256_i32 = arith.constant 256 : i32 loc(#loc1)
    %true = arith.constant true loc(#loc1)
    %0 = tt.get_program_id x : i32 loc(#loc2)
    %1 = tt.get_program_id y : i32 loc(#loc3)
    %2 = arith.divsi %1, %arg3 : i32 loc(#loc4)
    %3 = arith.remsi %1, %arg3 : i32 loc(#loc5)
    %4 = arith.muli %3, %arg24 : i32 loc(#loc6)
    %5 = arith.addi %2, %4 : i32 loc(#loc7)
    %6 = arith.muli %0, %c256_i32 : i32 loc(#loc8)
    %7 = arith.addi %5, %6 : i32 loc(#loc9)
    %8 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked1> loc(#loc10)
    %9 = tt.splat %6 : i32 -> tensor<128xi32, #blocked1> loc(#loc11)
    %10 = arith.addi %9, %8 : tensor<128xi32, #blocked1> loc(#loc11)
    %11 = tt.make_range {end = 256 : i32, start = 128 : i32} : tensor<128xi32, #blocked1> loc(#loc12)
    %12 = arith.addi %9, %11 : tensor<128xi32, #blocked1> loc(#loc13)
    %13 = arith.mulf %arg0, %cst_1 : f32 loc(#loc14)
    %14 = ttg.local_alloc : () -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc(#loc15)
    %15 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc15)
    ttng.init_barrier %15, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc15)
    ttng.barrier_expect %15, 32768, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc15)
    %16 = ttng.tensor_desc_to_tma_ptr %arg4 : !tt.tensordesc<tensor<128x128xf16, #shared>> to !tt.ptr<i8> loc(#loc15)
    ttng.async_tma_copy_global_to_local %16[%7, %c0_i32] %14, %15, %true : !tt.ptr<i8>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc(#loc15)
    ttng.wait_barrier %15, %c0_i32 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc15)
    ttng.inval_barrier %15 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc15)
    %17 = arith.addi %7, %c128_i32 : i32 loc(#loc16)
    %18 = ttg.local_alloc : () -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc(#loc17)
    %19 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc17)
    ttng.init_barrier %19, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc17)
    ttng.barrier_expect %19, 32768, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc17)
    %20 = ttng.tensor_desc_to_tma_ptr %arg4 : !tt.tensordesc<tensor<128x128xf16, #shared>> to !tt.ptr<i8> loc(#loc17)
    ttng.async_tma_copy_global_to_local %20[%17, %c0_i32] %18, %19, %true : !tt.ptr<i8>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc(#loc17)
    ttng.wait_barrier %19, %c0_i32 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc17)
    ttng.inval_barrier %19 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc17)
    %result = ttng.tmem_alloc : () -> !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc76)
    %21 = ttg.memdesc_subview %result[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc76)
    ttng.tmem_store %cst, %21, %true : tensor<128x128xf32, #blocked> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc76)
    %result_2 = ttng.tmem_alloc : () -> !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc77)
    %22 = ttg.memdesc_subview %result_2[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc77)
    ttng.tmem_store %cst, %22, %true : tensor<128x128xf32, #blocked> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc77)
    %23 = ttg.local_alloc : () -> !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> loc(#loc70)
    %24 = ttg.local_alloc : () -> !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(#loc71)
    %25 = ttg.memdesc_subview %24[%c0_i32] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.init_barrier %25, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    %26 = ttg.memdesc_subview %24[%c1_i32] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.init_barrier %26, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    %27 = ttg.local_alloc : () -> !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(#loc71)
    %28 = ttg.memdesc_subview %27[%c0_i32] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.init_barrier %28, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    %29 = ttg.memdesc_subview %27[%c1_i32] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.init_barrier %29, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    %result_3 = ttng.tmem_alloc : () -> !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc78)
    %30 = ttg.memdesc_subview %result_3[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc78)
    %result_4 = ttng.tmem_alloc : () -> !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc79)
    %31 = ttg.memdesc_subview %result_4[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc79)
    ttng.arrive_barrier %25, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc70)
    ttng.arrive_barrier %26, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc70)
    %32 = ttg.local_alloc : () -> !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> loc(#loc72)
    %33 = ttg.local_alloc : () -> !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(#loc71)
    %34 = ttg.memdesc_subview %33[%c0_i32] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.init_barrier %34, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    %35 = ttg.memdesc_subview %33[%c1_i32] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.init_barrier %35, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    %36 = ttg.local_alloc : () -> !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(#loc71)
    %37 = ttg.memdesc_subview %36[%c0_i32] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.init_barrier %37, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    %38 = ttg.memdesc_subview %36[%c1_i32] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.init_barrier %38, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.arrive_barrier %34, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc72)
    ttng.arrive_barrier %35, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc72)
    %39 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %40 = ttg.memdesc_subview %39[%c0_i32] : !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.init_barrier %40, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %41 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %42 = ttg.memdesc_subview %41[%c0_i32] : !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.init_barrier %42, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.arrive_barrier %42, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc78)
    %43 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %44 = ttg.memdesc_subview %43[%c0_i32] : !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.init_barrier %44, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %45 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %46 = ttg.memdesc_subview %45[%c0_i32] : !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.init_barrier %46, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.arrive_barrier %46, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc77)
    ttng.arrive_barrier %44, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc77)
    %47 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %48 = ttg.memdesc_subview %47[%c0_i32] : !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.init_barrier %48, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %49 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %50 = ttg.memdesc_subview %49[%c0_i32] : !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.init_barrier %50, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.arrive_barrier %47, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc77)
    %51 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %52 = ttg.memdesc_subview %51[%c0_i32] : !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.init_barrier %52, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %53 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %54 = ttg.memdesc_subview %53[%c0_i32] : !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.init_barrier %54, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.arrive_barrier %54, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc79)
    %55 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %56 = ttg.memdesc_subview %55[%c0_i32] : !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.init_barrier %56, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %57 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %58 = ttg.memdesc_subview %57[%c0_i32] : !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.init_barrier %58, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.arrive_barrier %58, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc76)
    ttng.arrive_barrier %56, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc76)
    %59 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %60 = ttg.memdesc_subview %59[%c0_i32] : !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.init_barrier %60, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %61 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    %62 = ttg.memdesc_subview %61[%c0_i32] : !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.init_barrier %62, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.arrive_barrier %59, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc76)
    %63 = ttg.local_alloc : () -> !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> loc(#loc80)
    %64 = ttg.local_alloc : () -> !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(#loc80)
    %65 = ttg.local_alloc : () -> !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(#loc80)
    %66 = ttg.memdesc_subview %63[%c0_i32, %c0_i32] : !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> loc(#loc80)
    ttg.local_store %cst_0, %66 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> loc(#loc80)
    %67 = ttg.memdesc_subview %64[%c0_i32] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    %68 = ttg.memdesc_subview %65[%c0_i32] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    ttng.init_barrier %67, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    ttng.init_barrier %68, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    ttng.arrive_barrier %67, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    %69 = ttg.memdesc_subview %64[%c1_i32] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    %70 = ttg.memdesc_subview %65[%c1_i32] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    ttng.init_barrier %69, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    ttng.init_barrier %70, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    ttng.arrive_barrier %70, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    %71 = ttg.memdesc_subview %64[%c2_i32] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    %72 = ttg.memdesc_subview %65[%c2_i32] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    ttng.init_barrier %71, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    ttng.init_barrier %72, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    ttng.arrive_barrier %72, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
    %73 = ttg.local_alloc : () -> !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> loc(#loc81)
    %74 = ttg.local_alloc : () -> !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(#loc81)
    %75 = ttg.local_alloc : () -> !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(#loc81)
    %76 = ttg.memdesc_subview %73[%c0_i32, %c0_i32] : !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> loc(#loc81)
    ttg.local_store %cst_0, %76 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> loc(#loc81)
    %77 = ttg.memdesc_subview %74[%c0_i32] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    %78 = ttg.memdesc_subview %75[%c0_i32] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    ttng.init_barrier %77, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    ttng.init_barrier %78, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    ttng.arrive_barrier %77, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    %79 = ttg.memdesc_subview %74[%c1_i32] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    %80 = ttg.memdesc_subview %75[%c1_i32] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    ttng.init_barrier %79, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    ttng.init_barrier %80, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    ttng.arrive_barrier %80, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    %81 = ttg.memdesc_subview %74[%c2_i32] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    %82 = ttg.memdesc_subview %75[%c2_i32] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    ttng.init_barrier %81, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    ttng.init_barrier %82, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    ttng.arrive_barrier %82, 2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
    %83 = ttg.local_alloc : () -> !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(#loc71)
    %84 = ttg.local_alloc : () -> !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(#loc71)
    %85 = ttg.local_alloc : () -> !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(#loc71)
    %86 = ttg.local_alloc : () -> !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(#loc71)
    %87:4 = ttg.warp_specialize(%24, %27, %23, %33, %36, %32, %42, %14, %30, %40, %63, %64, %65, %44, %49, %22, %46, %47, %54, %18, %31, %52, %73, %74, %75, %56, %61, %21, %58, %59, %arg24, %5, %arg9, %arg14, %84, %83, %86, %85, %13) attributes {requestedRegisters = array<i32: 32, 32, 192, 192>}
    default {
      %115:5 = scf.for %arg25 = %c0_i32 to %arg24 step %c128_i32 iter_args(%arg26 = %c0_i32, %arg27 = %c0_i32, %arg28 = %c0_i32, %arg29 = %c-1_i32, %arg30 = %c0_i32) -> (i32, i32, i32, i32, i32)  : i32 {
        %116 = arith.xori %arg26, %c1_i32 : i32 loc(#loc78)
        %117 = arith.addi %arg27, %c1_i32 : i32 loc(#loc80)
        %118 = arith.xori %arg28, %c1_i32 : i32 loc(#loc80)
        %119 = arith.cmpi eq, %117, %c3_i32 : i32 loc(#loc80)
        %120 = arith.select %119, %118, %arg28 : i32 loc(#loc80)
        %121 = arith.select %119, %c1_i32, %117 : i32 loc(#loc80)
        %122 = ttg.memdesc_subview %63[%121, %c0_i32] : !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> loc(#loc80)
        %123 = ttg.memdesc_subview %64[%121] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
        %124 = ttg.memdesc_subview %65[%121] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
        ttng.wait_barrier %123, %120 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
        %125 = ttg.local_load %122 : !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc80)
        ttng.arrive_barrier %124, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
        %126 = arith.addi %arg29, %c1_i32 : i32 loc(#loc80)
        %127 = arith.xori %arg30, %c1_i32 : i32 loc(#loc80)
        %128 = arith.cmpi eq, %126, %c3_i32 : i32 loc(#loc80)
        %129 = arith.select %128, %127, %arg30 : i32 loc(#loc80)
        %130 = arith.select %128, %c1_i32, %126 : i32 loc(#loc80)
        %131 = ttg.memdesc_subview %63[%130, %c0_i32] : !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> loc(#loc80)
        %132 = ttg.memdesc_subview %64[%130] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
        %133 = ttg.memdesc_subview %65[%130] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
        ttng.wait_barrier %132, %129 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
        %134 = ttg.local_load %131 : !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc80)
        ttng.arrive_barrier %133, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
        %135 = arith.subf %134, %125 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc82)
        %136 = math.exp2 %135 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc83)
        ttng.wait_barrier %46, %arg26 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc77)
        %137 = ttng.tmem_subslice %22 {N = 0 : i32} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc77)
        %138 = ttng.tmem_subslice %22 {N = 64 : i32} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc77)
        %139 = tt.expand_dims %136 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<128x1xf32, #blocked2> loc(#loc84)
        %140 = tt.broadcast %139 : tensor<128x1xf32, #blocked2> -> tensor<128x64xf32, #blocked2> loc(#loc85)
        %result_7 = ttng.tmem_load %137 : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x64xf32, #blocked2> loc(#loc77)
        %141 = arith.mulf %result_7, %140 : tensor<128x64xf32, #blocked2> loc(#loc85)
        ttng.tmem_store %141, %137, %true : tensor<128x64xf32, #blocked2> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc77)
        %result_8 = ttng.tmem_load %138 : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x64xf32, #blocked2> loc(#loc77)
        %142 = arith.mulf %result_8, %140 : tensor<128x64xf32, #blocked2> loc(#loc86)
        ttng.tmem_store %142, %138, %true : tensor<128x64xf32, #blocked2> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc77)
        ttng.arrive_barrier %44, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc77)
        %143 = ttg.memdesc_subview %73[%121, %c0_i32] : !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> loc(#loc81)
        %144 = ttg.memdesc_subview %74[%121] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
        %145 = ttg.memdesc_subview %75[%121] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
        ttng.wait_barrier %144, %120 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
        %146 = ttg.local_load %143 : !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc81)
        ttng.arrive_barrier %145, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
        %147 = ttg.memdesc_subview %73[%130, %c0_i32] : !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> loc(#loc81)
        %148 = ttg.memdesc_subview %74[%130] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
        %149 = ttg.memdesc_subview %75[%130] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
        ttng.wait_barrier %148, %129 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
        %150 = ttg.local_load %147 : !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc81)
        ttng.arrive_barrier %149, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
        %151 = arith.subf %150, %146 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc87)
        %152 = math.exp2 %151 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc88)
        ttng.wait_barrier %58, %arg26 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc76)
        %153 = ttng.tmem_subslice %21 {N = 0 : i32} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc76)
        %154 = ttng.tmem_subslice %21 {N = 64 : i32} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc76)
        %155 = tt.expand_dims %152 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<128x1xf32, #blocked2> loc(#loc89)
        %156 = tt.broadcast %155 : tensor<128x1xf32, #blocked2> -> tensor<128x64xf32, #blocked2> loc(#loc90)
        %result_9 = ttng.tmem_load %153 : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x64xf32, #blocked2> loc(#loc76)
        %157 = arith.mulf %result_9, %156 : tensor<128x64xf32, #blocked2> loc(#loc90)
        ttng.tmem_store %157, %153, %true : tensor<128x64xf32, #blocked2> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc76)
        %result_10 = ttng.tmem_load %154 : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x64xf32, #blocked2> loc(#loc76)
        %158 = arith.mulf %result_10, %156 : tensor<128x64xf32, #blocked2> loc(#loc91)
        ttng.tmem_store %158, %154, %true : tensor<128x64xf32, #blocked2> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc76)
        ttng.arrive_barrier %56, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc76)
        scf.yield %116, %121, %120, %130, %129 : i32, i32, i32, i32, i32 loc(#loc73)
      } {tt.disallow_acc_multi_buffer, tt.divisibility_arg1 = dense<128> : tensor<1xi32>, tt.warp_specialize} loc(#loc71)
      ttg.warp_yield %115#0, %115#0, %115#0, %115#0 : i32, i32, i32, i32 loc(#loc71)
    }
    partition0(%arg25: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg26: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg27: !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> loc(callsite(#loc22 at #loc20)), %arg28: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg29: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg30: !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> loc(callsite(#loc25 at #loc20)), %arg31: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg32: !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":475:21), %arg33: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc24 at #loc69)), %arg34: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg35: !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc69)), %arg36: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc69)), %arg37: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc69)), %arg38: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg39: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg40: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc18 at #loc69)), %arg41: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg42: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg43: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg44: !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":476:21), %arg45: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc24 at #loc68)), %arg46: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg47: !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc68)), %arg48: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc68)), %arg49: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc68)), %arg50: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg51: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg52: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc18 at #loc68)), %arg53: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg54: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg55: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg56: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":457:23), %arg57: !tt.tensordesc<tensor<128x128xf16, #shared>> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg58: !tt.tensordesc<tensor<128x128xf16, #shared>> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg59: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg60: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg61: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg62: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg63: f32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":473:16)) num_warps(1) {
      %c2_i32_7 = arith.constant 2 : i32 loc(#loc1)
      %c1_i32_8 = arith.constant 1 : i32 loc(#loc1)
      %c128_i32_9 = arith.constant 128 : i32 loc(#loc1)
      %c0_i32_10 = arith.constant 0 : i32 loc(#loc1)
      %true_11 = arith.constant true loc(#loc1)
      %false = arith.constant false loc(#loc1)
      %115 = arith.cmpi sgt, %arg55, %c0_i32_10 : i32 loc(#loc71)
      %116 = ttg.memdesc_subview %arg25[%c0_i32_10] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
      %117 = ttg.memdesc_subview %arg26[%c0_i32_10] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
      %118 = ttg.memdesc_subview %arg27[%c0_i32_10, %c0_i32_10, %c0_i32_10] : !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> loc(#loc70)
      %119 = ttg.memdesc_trans %118 {order = array<i32: 1, 0>} : !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> -> !ttg.memdesc<128x128xf16, #shared2, #smem, mutable> loc(#loc74)
      ttng.wait_barrier %117, %c0_i32_10, %115 {ttg.assigned_cluster = 1 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc70)
      ttng.wait_barrier %arg31, %c0_i32_10, %115 {ttg.assigned_cluster = 1 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc78)
      ttng.tc_gen5_mma %arg32, %119, %arg33, %false, %115, %116[%true_11], %arg34[%true_11] {tt.self_latency = 1 : i32, ttg.assigned_cluster = 1 : i32} : !ttg.memdesc<128x128xf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xf16, #shared2, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc78)
      ttng.wait_barrier %arg43, %c0_i32_10, %115 {ttg.assigned_cluster = 3 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc79)
      ttng.tc_gen5_mma %arg44, %119, %arg45, %false, %115, %116[%true_11], %arg46[%true_11] {tt.self_latency = 1 : i32, ttg.assigned_cluster = 3 : i32} : !ttg.memdesc<128x128xf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xf16, #shared2, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc79)
      %120:3 = scf.for %arg64 = %c0_i32_10 to %arg55 step %c128_i32_9 iter_args(%arg65 = %c0_i32_10, %arg66 = %c0_i32_10, %arg67 = %c0_i32_10) -> (i32, i32, i32)  : i32 {
        %121 = arith.subi %arg55, %c128_i32_9 : i32 loc(#loc71)
        %122 = arith.cmpi slt, %arg64, %121 : i32 loc(#loc71)
        %123 = ttg.memdesc_subview %arg28[%arg65] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
        %124 = ttg.memdesc_subview %arg29[%arg65] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
        %125 = ttg.memdesc_subview %arg30[%arg65, %c0_i32_10, %c0_i32_10] : !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> loc(#loc72)
        %126 = arith.xori %arg67, %c1_i32_8 : i32 loc(#loc77)
        %127 = "ttg.memdesc_reinterpret"(%arg33) : (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x128xf16, #tmem2, #ttng.tensor_memory, mutable> loc(#loc77)
        ttng.wait_barrier %124, %arg66 {ttg.assigned_cluster = 0 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc72)
        ttng.wait_barrier %arg38, %126, %true_11 {ttg.assigned_cluster = 0 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc77)
        ttng.wait_barrier %arg39, %arg67 {ttg.assigned_cluster = 0 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc77)
        ttng.tc_gen5_mma %127, %125, %arg40, %true_11, %true_11, %123[%true_11], %arg41[%true_11], %arg42[%true_11] {tt.self_latency = 1 : i32, ttg.assigned_cluster = 0 : i32} : !ttg.memdesc<128x128xf16, #tmem2, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc77)
        %128 = arith.addi %arg65, %c1_i32_8 : i32 loc(#loc72)
        %129 = arith.xori %arg66, %c1_i32_8 : i32 loc(#loc72)
        %130 = arith.cmpi eq, %128, %c2_i32_7 : i32 loc(#loc72)
        %131 = arith.select %130, %c0_i32_10, %128 : i32 loc(#loc70)
        %132 = arith.select %130, %129, %arg66 : i32 loc(#loc70)
        %133 = ttg.memdesc_subview %arg25[%131] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
        %134 = ttg.memdesc_subview %arg26[%131] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
        %135 = ttg.memdesc_subview %arg27[%131, %c0_i32_10, %c0_i32_10] : !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> loc(#loc70)
        %136 = ttg.memdesc_trans %135 {order = array<i32: 1, 0>} : !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> -> !ttg.memdesc<128x128xf16, #shared2, #smem, mutable> loc(#loc74)
        ttng.wait_barrier %134, %132, %122 {ttg.assigned_cluster = 1 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc70)
        ttng.wait_barrier %arg31, %126, %122 {ttg.assigned_cluster = 1 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc78)
        ttng.tc_gen5_mma %arg32, %136, %arg33, %false, %122, %133[%true_11], %arg34[%true_11] {tt.self_latency = 1 : i32, ttg.assigned_cluster = 1 : i32} : !ttg.memdesc<128x128xf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xf16, #shared2, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc78)
        %137 = "ttg.memdesc_reinterpret"(%arg45) : (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x128xf16, #tmem2, #ttng.tensor_memory, mutable> loc(#loc76)
        ttng.wait_barrier %arg50, %126, %true_11 {ttg.assigned_cluster = 2 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc76)
        ttng.wait_barrier %arg51, %arg67 {ttg.assigned_cluster = 2 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc76)
        ttng.tc_gen5_mma %137, %125, %arg52, %true_11, %true_11, %123[%true_11], %arg53[%true_11], %arg54[%true_11] {tt.self_latency = 1 : i32, ttg.assigned_cluster = 2 : i32} : !ttg.memdesc<128x128xf16, #tmem2, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc76)
        ttng.wait_barrier %arg43, %126, %122 {ttg.assigned_cluster = 3 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc79)
        ttng.tc_gen5_mma %arg44, %136, %arg45, %false, %122, %133[%true_11], %arg46[%true_11] {tt.self_latency = 1 : i32, ttg.assigned_cluster = 3 : i32} : !ttg.memdesc<128x128xf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xf16, #shared2, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc79)
        scf.yield %131, %132, %126 : i32, i32, i32 loc(#loc71)
      } {tt.disallow_acc_multi_buffer, tt.divisibility_arg1 = dense<128> : tensor<1xi32>, tt.warp_specialize} loc(#loc71)
      ttg.warp_return loc(#loc71)
    }
    partition1(%arg25: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg26: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg27: !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> loc(callsite(#loc22 at #loc20)), %arg28: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg29: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg30: !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> loc(callsite(#loc25 at #loc20)), %arg31: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg32: !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":475:21), %arg33: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc24 at #loc69)), %arg34: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg35: !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc69)), %arg36: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc69)), %arg37: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc69)), %arg38: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg39: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg40: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc18 at #loc69)), %arg41: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg42: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg43: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg44: !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":476:21), %arg45: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc24 at #loc68)), %arg46: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg47: !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc68)), %arg48: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc68)), %arg49: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc68)), %arg50: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg51: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg52: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc18 at #loc68)), %arg53: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg54: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg55: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg56: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":457:23), %arg57: !tt.tensordesc<tensor<128x128xf16, #shared>> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg58: !tt.tensordesc<tensor<128x128xf16, #shared>> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg59: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg60: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg61: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg62: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg63: f32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":473:16)) num_warps(2) {
      %c256_i32_7 = arith.constant 256 : i32 loc(#loc1)
      %c2_i32_8 = arith.constant 2 : i32 loc(#loc1)
      %c1_i32_9 = arith.constant 1 : i32 loc(#loc1)
      %c128_i32_10 = arith.constant 128 : i32 loc(#loc1)
      %c0_i32_11 = arith.constant 0 : i32 loc(#loc1)
      %true_12 = arith.constant true loc(#loc1)
      %115 = arith.cmpi sgt, %arg55, %c0_i32_11 : i32 loc(#loc71)
      %116 = ttg.memdesc_subview %arg25[%c0_i32_11] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
      ttng.wait_barrier %116, %c0_i32_11, %115 {ttg.assigned_cluster = 2 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc70)
      %117 = ttg.memdesc_subview %arg26[%c0_i32_11] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
      ttng.barrier_expect %117, 32768 {ttg.assigned_cluster = 2 : i32}, %115 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc70)
      %118 = ttg.memdesc_subview %arg27[%c0_i32_11, %c0_i32_11, %c0_i32_11] : !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> loc(#loc70)
      %119 = ttng.tensor_desc_to_tma_ptr %arg57 {ttg.assigned_cluster = 2 : i32} : !tt.tensordesc<tensor<128x128xf16, #shared>> to !tt.ptr<i8> loc(#loc70)
      ttng.async_tma_copy_global_to_local %119[%arg56, %c0_i32_11] %118, %117, %115 {ttg.assigned_cluster = 2 : i32} : !tt.ptr<i8>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> loc(#loc70)
      %120 = arith.cmpi sgt, %arg55, %c128_i32_10 : i32 loc(#loc71)
      %121 = arith.addi %arg56, %c128_i32_10 : i32 loc(#loc75)
      %122 = ttg.memdesc_subview %arg25[%c1_i32_9] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
      ttng.wait_barrier %122, %c0_i32_11, %120 {ttg.assigned_cluster = 2 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc70)
      %123 = ttg.memdesc_subview %arg26[%c1_i32_9] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
      ttng.barrier_expect %123, 32768 {ttg.assigned_cluster = 2 : i32}, %120 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc70)
      %124 = ttg.memdesc_subview %arg27[%c1_i32_9, %c0_i32_11, %c0_i32_11] : !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> loc(#loc70)
      ttng.async_tma_copy_global_to_local %119[%121, %c0_i32_11] %124, %123, %120 {ttg.assigned_cluster = 2 : i32} : !tt.ptr<i8>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> loc(#loc70)
      %125:6 = scf.for %arg64 = %c0_i32_11 to %arg55 step %c128_i32_10 iter_args(%arg65 = %121, %arg66 = %c1_i32_9, %arg67 = %c0_i32_11, %arg68 = %c0_i32_11, %arg69 = %c0_i32_11, %arg70 = %arg56) -> (i32, i32, i32, i32, i32, i32)  : i32 {
        %126 = arith.subi %arg55, %c256_i32_7 : i32 loc(#loc71)
        %127 = arith.cmpi slt, %arg64, %126 : i32 loc(#loc71)
        %128 = ttg.memdesc_subview %arg28[%arg68] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
        ttng.wait_barrier %128, %arg69 {ttg.assigned_cluster = 0 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc72)
        %129 = ttg.memdesc_subview %arg29[%arg68] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
        ttng.barrier_expect %129, 32768 {ttg.assigned_cluster = 0 : i32}, %true_12 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc72)
        %130 = ttg.memdesc_subview %arg30[%arg68, %c0_i32_11, %c0_i32_11] : !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> loc(#loc72)
        %131 = ttng.tensor_desc_to_tma_ptr %arg58 {ttg.assigned_cluster = 0 : i32} : !tt.tensordesc<tensor<128x128xf16, #shared>> to !tt.ptr<i8> loc(#loc72)
        ttng.async_tma_copy_global_to_local %131[%arg70, %c0_i32_11] %130, %129, %true_12 {ttg.assigned_cluster = 0 : i32} : !tt.ptr<i8>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> loc(#loc72)
        %132 = arith.addi %arg68, %c1_i32_9 : i32 loc(#loc72)
        %133 = arith.xori %arg69, %c1_i32_9 : i32 loc(#loc72)
        %134 = arith.cmpi eq, %132, %c2_i32_8 : i32 loc(#loc72)
        %135 = arith.select %134, %c0_i32_11, %132 : i32 loc(#loc72)
        %136 = arith.select %134, %133, %arg69 : i32 loc(#loc72)
        %137 = arith.addi %arg65, %c128_i32_10 : i32 loc(#loc75)
        %138 = arith.addi %arg66, %c1_i32_9 : i32 loc(#loc70)
        %139 = arith.xori %arg67, %c1_i32_9 : i32 loc(#loc70)
        %140 = arith.cmpi eq, %138, %c2_i32_8 : i32 loc(#loc70)
        %141 = arith.select %140, %c0_i32_11, %138 : i32 loc(#loc70)
        %142 = arith.select %140, %139, %arg67 : i32 loc(#loc70)
        %143 = ttg.memdesc_subview %arg25[%141] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
        ttng.wait_barrier %143, %142, %127 {ttg.assigned_cluster = 2 : i32} : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc70)
        %144 = ttg.memdesc_subview %arg26[%141] : !ttg.memdesc<2xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
        ttng.barrier_expect %144, 32768 {ttg.assigned_cluster = 2 : i32}, %127 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc70)
        %145 = ttg.memdesc_subview %arg27[%141, %c0_i32_11, %c0_i32_11] : !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> loc(#loc70)
        ttng.async_tma_copy_global_to_local %119[%137, %c0_i32_11] %145, %144, %127 {ttg.assigned_cluster = 2 : i32} : !tt.ptr<i8>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable, 2x128x128> loc(#loc70)
        scf.yield %137, %141, %142, %135, %136, %arg65 : i32, i32, i32, i32, i32, i32 loc(#loc71)
      } {tt.disallow_acc_multi_buffer, tt.divisibility_arg1 = dense<128> : tensor<1xi32>, tt.warp_specialize} loc(#loc71)
      ttg.warp_return loc(#loc71)
    }
    partition2(%arg25: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg26: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg27: !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> loc(callsite(#loc22 at #loc20)), %arg28: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg29: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg30: !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> loc(callsite(#loc25 at #loc20)), %arg31: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg32: !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":475:21), %arg33: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc24 at #loc69)), %arg34: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg35: !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc69)), %arg36: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc69)), %arg37: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc69)), %arg38: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg39: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg40: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc18 at #loc69)), %arg41: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg42: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg43: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg44: !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":476:21), %arg45: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc24 at #loc68)), %arg46: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg47: !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc68)), %arg48: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc68)), %arg49: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc68)), %arg50: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg51: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg52: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc18 at #loc68)), %arg53: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg54: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg55: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg56: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":457:23), %arg57: !tt.tensordesc<tensor<128x128xf16, #shared>> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg58: !tt.tensordesc<tensor<128x128xf16, #shared>> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg59: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg60: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg61: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg62: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg63: f32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":473:16)) num_warps(4) {
      %c3_i32_7 = arith.constant 3 : i32 loc(#loc1)
      %c1_i32_8 = arith.constant 1 : i32 loc(#loc1)
      %cst_9 = arith.constant dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
      %cst_10 = arith.constant dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
      %c128_i32_11 = arith.constant 128 : i32 loc(#loc1)
      %c0_i32_12 = arith.constant 0 : i32 loc(#loc1)
      %true_13 = arith.constant true loc(#loc1)
      %115 = tt.splat %arg63 : f32 -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc92)
      %116 = tt.splat %arg63 : f32 -> tensor<128x128xf32, #blocked> loc(#loc93)
      %117:5 = scf.for %arg64 = %c0_i32_12 to %arg55 step %c128_i32_11 iter_args(%arg65 = %cst_10, %arg66 = %cst_9, %arg67 = %c0_i32_12, %arg68 = %c0_i32_12, %arg69 = %c0_i32_12) -> (tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, i32, i32, i32)  : i32 {
        ttng.wait_barrier %arg34, %arg67 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc78)
        %result_14 = ttng.tmem_load %arg33 : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> -> tensor<128x128xf32, #blocked> loc(#loc78)
        ttng.arrive_barrier %arg31, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc78)
        %118 = arith.xori %arg67, %c1_i32_8 : i32 loc(#loc78)
        %119 = "tt.reduce"(%result_14) <{axis = 1 : i32}> ({
        ^bb0(%arg70: f32 loc(callsite(#loc1 at #loc94)), %arg71: f32 loc(callsite(#loc1 at #loc94))):
          %142 = arith.maxnumf %arg70, %arg71 : f32 loc(#loc120)
          tt.reduce.return %142 : f32 loc(#loc112)
        }) : (tensor<128x128xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc112)
        %120 = arith.mulf %119, %115 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc92)
        %121 = arith.maxnumf %arg66, %120 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc80)
        %122 = arith.addi %arg68, %c1_i32_8 : i32 loc(#loc80)
        %123 = arith.xori %arg69, %c1_i32_8 : i32 loc(#loc80)
        %124 = arith.cmpi eq, %122, %c3_i32_7 : i32 loc(#loc80)
        %125 = arith.select %124, %123, %arg69 : i32 loc(#loc80)
        %126 = arith.select %124, %c1_i32_8, %122 : i32 loc(#loc80)
        %127 = ttg.memdesc_subview %arg35[%126, %c0_i32_12] : !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> loc(#loc80)
        %128 = ttg.memdesc_subview %arg36[%126] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
        %129 = ttg.memdesc_subview %arg37[%126] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
        ttng.wait_barrier %129, %125 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
        ttg.local_store %121, %127 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> loc(#loc80)
        ttng.arrive_barrier %128, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc80)
        %130 = arith.mulf %result_14, %116 : tensor<128x128xf32, #blocked> loc(#loc93)
        %131 = tt.expand_dims %121 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc95)
        %132 = tt.broadcast %131 : tensor<128x1xf32, #blocked> -> tensor<128x128xf32, #blocked> loc(#loc96)
        %133 = arith.subf %130, %132 : tensor<128x128xf32, #blocked> loc(#loc96)
        %134 = math.exp2 %133 : tensor<128x128xf32, #blocked> loc(#loc97)
        %135 = arith.subf %arg66, %121 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc82)
        %136 = math.exp2 %135 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc83)
        %137 = "tt.reduce"(%134) <{axis = 1 : i32}> ({
        ^bb0(%arg70: f32 loc(callsite(#loc1 at #loc98)), %arg71: f32 loc(callsite(#loc1 at #loc98))):
          %142 = arith.addf %arg70, %arg71 : f32 loc(#loc121)
          tt.reduce.return %142 : f32 loc(#loc114)
        }) : (tensor<128x128xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc114)
        %138 = arith.truncf %134 : tensor<128x128xf32, #blocked> to tensor<128x128xf16, #blocked> loc(#loc99)
        %139 = "ttg.memdesc_reinterpret"(%arg33) : (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x128xf16, #tmem2, #ttng.tensor_memory, mutable> loc(#loc77)
        ttng.wait_barrier %arg42, %arg67 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc77)
        ttng.tmem_store %138, %139, %true_13 : tensor<128x128xf16, #blocked> -> !ttg.memdesc<128x128xf16, #tmem2, #ttng.tensor_memory, mutable> loc(#loc77)
        ttng.arrive_barrier %arg39, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc77)
        %140 = arith.mulf %arg65, %136 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc100)
        %141 = arith.addf %140, %137 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc101)
        scf.yield %141, %121, %118, %126, %125 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, i32, i32, i32 loc(#loc73)
      } {tt.disallow_acc_multi_buffer, tt.divisibility_arg1 = dense<128> : tensor<1xi32>, tt.warp_specialize} loc(#loc71)
      ttg.local_store %117#0, %arg59 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(#loc71)
      ttg.local_store %117#1, %arg60 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(#loc71)
      ttg.warp_return loc(#loc71)
    }
    partition3(%arg25: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg26: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg27: !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> loc(callsite(#loc22 at #loc20)), %arg28: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg29: !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg30: !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> loc(callsite(#loc25 at #loc20)), %arg31: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg32: !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":475:21), %arg33: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc24 at #loc69)), %arg34: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg35: !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc69)), %arg36: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc69)), %arg37: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc69)), %arg38: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg39: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg40: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc18 at #loc69)), %arg41: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg42: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg43: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg44: !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":476:21), %arg45: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc24 at #loc68)), %arg46: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg47: !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc68)), %arg48: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc68)), %arg49: !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(callsite(#loc26 at #loc68)), %arg50: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg51: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg52: !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(callsite(#loc18 at #loc68)), %arg53: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg54: !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg55: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg56: i32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":457:23), %arg57: !tt.tensordesc<tensor<128x128xf16, #shared>> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg58: !tt.tensordesc<tensor<128x128xf16, #shared>> loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":441:0), %arg59: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg60: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg61: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg62: !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(callsite(#loc23 at #loc20)), %arg63: f32 loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":473:16)) num_warps(4) {
      %c3_i32_7 = arith.constant 3 : i32 loc(#loc1)
      %c1_i32_8 = arith.constant 1 : i32 loc(#loc1)
      %cst_9 = arith.constant dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
      %cst_10 = arith.constant dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
      %c128_i32_11 = arith.constant 128 : i32 loc(#loc1)
      %c0_i32_12 = arith.constant 0 : i32 loc(#loc1)
      %true_13 = arith.constant true loc(#loc1)
      %115 = tt.splat %arg63 : f32 -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc92)
      %116 = tt.splat %arg63 : f32 -> tensor<128x128xf32, #blocked> loc(#loc93)
      %117:5 = scf.for %arg64 = %c0_i32_12 to %arg55 step %c128_i32_11 iter_args(%arg65 = %cst_10, %arg66 = %cst_9, %arg67 = %c0_i32_12, %arg68 = %c0_i32_12, %arg69 = %c0_i32_12) -> (tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, i32, i32, i32)  : i32 {
        ttng.wait_barrier %arg46, %arg67 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc79)
        %result_14 = ttng.tmem_load %arg45 : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> -> tensor<128x128xf32, #blocked> loc(#loc79)
        ttng.arrive_barrier %arg43, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc79)
        %118 = arith.xori %arg67, %c1_i32_8 : i32 loc(#loc79)
        %119 = "tt.reduce"(%result_14) <{axis = 1 : i32}> ({
        ^bb0(%arg70: f32 loc(callsite(#loc1 at #loc102)), %arg71: f32 loc(callsite(#loc1 at #loc102))):
          %142 = arith.maxnumf %arg70, %arg71 : f32 loc(#loc122)
          tt.reduce.return %142 : f32 loc(#loc116)
        }) : (tensor<128x128xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc116)
        %120 = arith.mulf %119, %115 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc103)
        %121 = arith.maxnumf %arg66, %120 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc81)
        %122 = arith.addi %arg68, %c1_i32_8 : i32 loc(#loc81)
        %123 = arith.xori %arg69, %c1_i32_8 : i32 loc(#loc81)
        %124 = arith.cmpi eq, %122, %c3_i32_7 : i32 loc(#loc81)
        %125 = arith.select %124, %123, %arg69 : i32 loc(#loc81)
        %126 = arith.select %124, %c1_i32_8, %122 : i32 loc(#loc81)
        %127 = ttg.memdesc_subview %arg47[%126, %c0_i32_12] : !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> loc(#loc81)
        %128 = ttg.memdesc_subview %arg48[%126] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
        %129 = ttg.memdesc_subview %arg49[%126] : !ttg.memdesc<3xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
        ttng.wait_barrier %129, %125 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
        ttg.local_store %121, %127 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable, 3x128> loc(#loc81)
        ttng.arrive_barrier %128, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc81)
        %130 = arith.mulf %result_14, %116 : tensor<128x128xf32, #blocked> loc(#loc104)
        %131 = tt.expand_dims %121 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc105)
        %132 = tt.broadcast %131 : tensor<128x1xf32, #blocked> -> tensor<128x128xf32, #blocked> loc(#loc106)
        %133 = arith.subf %130, %132 : tensor<128x128xf32, #blocked> loc(#loc106)
        %134 = math.exp2 %133 : tensor<128x128xf32, #blocked> loc(#loc107)
        %135 = arith.subf %arg66, %121 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc87)
        %136 = math.exp2 %135 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc88)
        %137 = "tt.reduce"(%134) <{axis = 1 : i32}> ({
        ^bb0(%arg70: f32 loc(callsite(#loc1 at #loc108)), %arg71: f32 loc(callsite(#loc1 at #loc108))):
          %142 = arith.addf %arg70, %arg71 : f32 loc(#loc123)
          tt.reduce.return %142 : f32 loc(#loc118)
        }) : (tensor<128x128xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc118)
        %138 = arith.truncf %134 : tensor<128x128xf32, #blocked> to tensor<128x128xf16, #blocked> loc(#loc109)
        %139 = "ttg.memdesc_reinterpret"(%arg45) : (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x128xf16, #tmem2, #ttng.tensor_memory, mutable> loc(#loc76)
        ttng.wait_barrier %arg54, %arg67 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc76)
        ttng.tmem_store %138, %139, %true_13 : tensor<128x128xf16, #blocked> -> !ttg.memdesc<128x128xf16, #tmem2, #ttng.tensor_memory, mutable> loc(#loc76)
        ttng.arrive_barrier %arg51, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc76)
        %140 = arith.mulf %arg65, %136 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc110)
        %141 = arith.addf %140, %137 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc111)
        scf.yield %141, %121, %118, %126, %125 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, i32, i32, i32 loc(#loc73)
      } {tt.disallow_acc_multi_buffer, tt.divisibility_arg1 = dense<128> : tensor<1xi32>, tt.warp_specialize} loc(#loc71)
      ttg.local_store %117#0, %arg61 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(#loc71)
      ttg.local_store %117#1, %arg62 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(#loc71)
      ttg.warp_return loc(#loc71)
    } : (!ttg.memdesc<2xi64, #shared1, #smem, mutable>, !ttg.memdesc<2xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable>, !ttg.memdesc<2xi64, #shared1, #smem, mutable>, !ttg.memdesc<2xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<128x128xf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<3x128xf32, #shared1, #smem, mutable>, !ttg.memdesc<3xi64, #shared1, #smem, mutable>, !ttg.memdesc<3xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<128x128xf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<3x128xf32, #shared1, #smem, mutable>, !ttg.memdesc<3xi64, #shared1, #smem, mutable>, !ttg.memdesc<3xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, i32, i32, !tt.tensordesc<tensor<128x128xf16, #shared>>, !tt.tensordesc<tensor<128x128xf16, #shared>>, !ttg.memdesc<128xf32, #shared1, #smem, mutable>, !ttg.memdesc<128xf32, #shared1, #smem, mutable>, !ttg.memdesc<128xf32, #shared1, #smem, mutable>, !ttg.memdesc<128xf32, #shared1, #smem, mutable>, f32) -> (i32, i32, i32, i32) loc(#loc71)
    %88 = ttg.local_load %86 : !ttg.memdesc<128xf32, #shared1, #smem, mutable> -> tensor<128xf32, #blocked1> loc(#loc71)
    %89 = ttg.local_load %86 : !ttg.memdesc<128xf32, #shared1, #smem, mutable> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc71)
    ttg.local_dealloc %86 : !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(#loc71)
    %90 = ttg.local_load %85 : !ttg.memdesc<128xf32, #shared1, #smem, mutable> -> tensor<128xf32, #blocked1> loc(#loc71)
    ttg.local_dealloc %85 : !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(#loc71)
    %91 = ttg.local_load %84 : !ttg.memdesc<128xf32, #shared1, #smem, mutable> -> tensor<128xf32, #blocked1> loc(#loc71)
    %92 = ttg.local_load %84 : !ttg.memdesc<128xf32, #shared1, #smem, mutable> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc71)
    ttg.local_dealloc %84 : !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(#loc71)
    %93 = ttg.local_load %83 : !ttg.memdesc<128xf32, #shared1, #smem, mutable> -> tensor<128xf32, #blocked1> loc(#loc71)
    ttg.local_dealloc %83 : !ttg.memdesc<128xf32, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %73 : !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %74 : !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(#loc81)
    ttg.local_dealloc %75 : !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(#loc81)
    ttg.local_dealloc %63 : !ttg.memdesc<3x128xf32, #shared1, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %67 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc71)
    ttng.inval_barrier %68 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc71)
    ttng.inval_barrier %69 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc71)
    ttng.inval_barrier %70 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc71)
    ttng.inval_barrier %71 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc71)
    ttng.inval_barrier %72 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc71)
    ttg.local_dealloc %64 : !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(#loc80)
    ttg.local_dealloc %65 : !ttg.memdesc<3xi64, #shared1, #smem, mutable> loc(#loc80)
    ttng.inval_barrier %77 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc71)
    ttng.inval_barrier %78 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc71)
    ttng.inval_barrier %79 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc71)
    ttng.inval_barrier %80 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc71)
    ttng.inval_barrier %81 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc71)
    ttng.inval_barrier %82 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 3> loc(#loc71)
    ttng.wait_barrier %58, %87#3 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc76)
    ttng.inval_barrier %62 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %61 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %60 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %59 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %58 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %57 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %56 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %55 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.wait_barrier %54, %87#2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc79)
    ttng.inval_barrier %54 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %53 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %52 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %51 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.wait_barrier %46, %87#1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc77)
    ttng.inval_barrier %50 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %49 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %48 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %47 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %46 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %45 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %44 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %43 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.wait_barrier %42, %87#0 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc78)
    ttng.inval_barrier %42 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %41 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %40 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %39 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %37 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.inval_barrier %38 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttg.local_dealloc %36 : !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %34 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.inval_barrier %35 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttg.local_dealloc %33 : !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %32 : !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %28 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.inval_barrier %29 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttg.local_dealloc %27 : !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(#loc71)
    ttng.inval_barrier %25 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttng.inval_barrier %26 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2> loc(#loc71)
    ttg.local_dealloc %24 : !ttg.memdesc<2xi64, #shared1, #smem, mutable> loc(#loc71)
    ttg.local_dealloc %23 : !ttg.memdesc<2x128x128xf16, #shared, #smem, mutable> loc(#loc71)
    %94 = math.log2 %91 : tensor<128xf32, #blocked1> loc(#loc49)
    %95 = arith.addf %93, %94 : tensor<128xf32, #blocked1> loc(#loc50)
    %96 = tt.expand_dims %92 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc51)
    %97 = tt.broadcast %96 : tensor<128x1xf32, #blocked> -> tensor<128x128xf32, #blocked> loc(#loc52)
    %98 = arith.muli %1, %arg24 : i32 loc(#loc53)
    %99 = tt.addptr %arg1, %98 : !tt.ptr<f32>, i32 loc(#loc54)
    %100 = tt.splat %99 : !tt.ptr<f32> -> tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc55)
    %101 = tt.addptr %100, %10 : tensor<128x!tt.ptr<f32>, #blocked1>, tensor<128xi32, #blocked1> loc(#loc55)
    tt.store %101, %95 : tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc56)
    %result_5 = ttng.tmem_load %22 : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> -> tensor<128x128xf32, #blocked> loc(#loc77)
    %102 = arith.divf %result_5, %97 : tensor<128x128xf32, #blocked> loc(#loc52)
    %103 = arith.truncf %102 : tensor<128x128xf32, #blocked> to tensor<128x128xf16, #blocked> loc(#loc57)
    %104 = ttg.local_alloc %103 : (tensor<128x128xf16, #blocked>) -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc(#loc58)
    ttng.fence_async_shared {bCluster = false} loc(#loc58)
    %105 = ttng.tensor_desc_to_tma_ptr %arg19 : !tt.tensordesc<tensor<128x128xf16, #shared>> to !tt.ptr<i8> loc(#loc58)
    ttng.async_tma_copy_local_to_global %105[%7, %c0_i32] %104 : !tt.ptr<i8>, !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc(#loc58)
    ttng.async_tma_store_wait {pendings = 0 : i32} loc(#loc58)
    %106 = math.log2 %88 : tensor<128xf32, #blocked1> loc(#loc59)
    %107 = arith.addf %90, %106 : tensor<128xf32, #blocked1> loc(#loc60)
    %108 = tt.expand_dims %89 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc61)
    %109 = tt.broadcast %108 : tensor<128x1xf32, #blocked> -> tensor<128x128xf32, #blocked> loc(#loc62)
    %110 = tt.addptr %100, %12 : tensor<128x!tt.ptr<f32>, #blocked1>, tensor<128xi32, #blocked1> loc(#loc63)
    tt.store %110, %107 : tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc64)
    %result_6 = ttng.tmem_load %21 : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> -> tensor<128x128xf32, #blocked> loc(#loc76)
    %111 = arith.divf %result_6, %109 : tensor<128x128xf32, #blocked> loc(#loc62)
    %112 = arith.truncf %111 : tensor<128x128xf32, #blocked> to tensor<128x128xf16, #blocked> loc(#loc65)
    %113 = ttg.local_alloc %112 : (tensor<128x128xf16, #blocked>) -> !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc(#loc66)
    ttng.fence_async_shared {bCluster = false} loc(#loc66)
    %114 = ttng.tensor_desc_to_tma_ptr %arg19 : !tt.tensordesc<tensor<128x128xf16, #shared>> to !tt.ptr<i8> loc(#loc66)
    ttng.async_tma_copy_local_to_global %114[%17, %c0_i32] %113 : !tt.ptr<i8>, !ttg.memdesc<128x128xf16, #shared, #smem, mutable> loc(#loc66)
    ttng.async_tma_store_wait {pendings = 0 : i32} loc(#loc66)
    tt.return loc(#loc67)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":452:28)
#loc3 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":453:27)
#loc4 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":454:22)
#loc5 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":455:21)
#loc6 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":457:31)
#loc8 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":458:39)
#loc9 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":458:29)
#loc10 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":460:47)
#loc11 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":460:34)
#loc12 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":461:56)
#loc13 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":461:34)
#loc16 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":476:36)
#loc27 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":159:31)
#loc28 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":159:25)
#loc29 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":167:24)
#loc30 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":167:18)
#loc31 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":168:18)
#loc32 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":212:8)
#loc33 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":206:12)
#loc34 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":212:22)
#loc35 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":155:47)
#loc36 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":156:18)
#loc37 = loc("/home/mren/OpenSource2/triton/python/triton/language/standard.py":188:40)
#loc39 = loc("/home/mren/OpenSource2/triton/python/triton/language/standard.py":167:27)
#loc40 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":156:34)
#loc41 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":156:29)
#loc42 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":157:21)
#loc43 = loc("/home/mren/OpenSource2/triton/python/triton/language/standard.py":290:36)
#loc45 = loc("/home/mren/OpenSource2/triton/python/triton/language/standard.py":260:15)
#loc46 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":172:13)
#loc47 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":177:16)
#loc48 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":177:24)
#loc49 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":493:25)
#loc50 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":493:12)
#loc51 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":494:23)
#loc52 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":494:18)
#loc53 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":495:27)
#loc54 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":495:18)
#loc55 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":495:35)
#loc56 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":496:22)
#loc57 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":497:43)
#loc58 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":497:35)
#loc59 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":499:25)
#loc60 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":499:12)
#loc61 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":500:23)
#loc62 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":500:18)
#loc63 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":501:35)
#loc64 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":502:22)
#loc65 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":503:54)
#loc66 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":503:46)
#loc67 = loc("/home/mren/OpenSource2/triton/python/tutorials/06-fused-attention.py":503:4)
#loc73 = loc(callsite(#loc32 at #loc20))
#loc74 = loc(callsite(#loc33 at #loc20))
#loc75 = loc(callsite(#loc34 at #loc20))
#loc82 = loc(callsite(#loc27 at #loc69))
#loc83 = loc(callsite(#loc28 at #loc69))
#loc84 = loc(callsite(#loc29 at #loc69))
#loc85 = loc(callsite(#loc30 at #loc69))
#loc86 = loc(callsite(#loc31 at #loc69))
#loc87 = loc(callsite(#loc27 at #loc68))
#loc88 = loc(callsite(#loc28 at #loc68))
#loc89 = loc(callsite(#loc29 at #loc68))
#loc90 = loc(callsite(#loc30 at #loc68))
#loc91 = loc(callsite(#loc31 at #loc68))
#loc92 = loc(callsite(#loc35 at #loc69))
#loc93 = loc(callsite(#loc36 at #loc69))
#loc95 = loc(callsite(#loc40 at #loc69))
#loc96 = loc(callsite(#loc41 at #loc69))
#loc97 = loc(callsite(#loc42 at #loc69))
#loc99 = loc(callsite(#loc46 at #loc69))
#loc100 = loc(callsite(#loc47 at #loc69))
#loc101 = loc(callsite(#loc48 at #loc69))
#loc103 = loc(callsite(#loc35 at #loc68))
#loc104 = loc(callsite(#loc36 at #loc68))
#loc105 = loc(callsite(#loc40 at #loc68))
#loc106 = loc(callsite(#loc41 at #loc68))
#loc107 = loc(callsite(#loc42 at #loc68))
#loc109 = loc(callsite(#loc46 at #loc68))
#loc110 = loc(callsite(#loc47 at #loc68))
#loc111 = loc(callsite(#loc48 at #loc68))
#loc112 = loc(callsite(#loc37 at #loc94))
#loc114 = loc(callsite(#loc43 at #loc98))
#loc116 = loc(callsite(#loc37 at #loc102))
#loc118 = loc(callsite(#loc43 at #loc108))
#loc120 = loc(callsite(#loc39 at #loc112))
#loc121 = loc(callsite(#loc45 at #loc114))
#loc122 = loc(callsite(#loc39 at #loc116))
#loc123 = loc(callsite(#loc45 at #loc118))

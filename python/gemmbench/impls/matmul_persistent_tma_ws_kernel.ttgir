#loc = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":8:0)
#mma = #triton_gpu.nvidia_mma<{versionMajor = 3, versionMinor = 0, warpsPerCTA = [4, 2], instrShape = [16, 128, 16]}>
#shared = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 8, order = [1, 0], hasLeadingOffset = true}>
#shared1 = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0], hasLeadingOffset = false}>
module attributes {"triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warp-groups-per-cta" = 2 : i32, "triton_gpu.num-warps" = 8 : i32, triton_gpu.target = "cuda:90", "triton_gpu.threads-per-warp" = 32 : i32} {
  tt.func public @matmul_persistent_tma_ws_kernel(%arg0: !tt.ptr<i8, 0> {tt.nv_tma_desc = 1 : i32} loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":8:0), %arg1: !tt.ptr<i8, 0> {tt.nv_tma_desc = 1 : i32} loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":8:0), %arg2: !tt.ptr<i8, 0> {tt.nv_tma_desc = 1 : i32} loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":8:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":8:0), %arg4: i32 {tt.divisibility = 16 : i32} loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":8:0), %arg5: i32 {tt.divisibility = 16 : i32} loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":8:0), %arg6: i32 {tt.divisibility = 16 : i32} loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":8:0), %arg7: i32 {tt.divisibility = 16 : i32} loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":8:0), %arg8: i32 {tt.divisibility = 16 : i32} loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":8:0)) attributes {noinline = false} {
    %c-2_i32 = arith.constant -2 : i32 loc(#loc1)
    %c128_i32 = arith.constant 128 : i32 loc(#loc1)
    %cst = arith.constant {async_task_id = dense<1> : vector<1xi32>} dense<0.000000e+00> : tensor<64x256xf32, #mma> loc(#loc1)
    %true = arith.constant {async_task_id = dense<0> : vector<1xi32>} true loc(#loc1)
    %c3_i32 = arith.constant {async_task_id = dense<[0, 1]> : vector<2xi32>} 3 : i32 loc(#loc1)
    %c255_i32 = arith.constant {async_task_id = dense<[0, 1]> : vector<2xi32>} 255 : i32 loc(#loc1)
    %c256_i32 = arith.constant {async_task_id = dense<[0, 1]> : vector<2xi32>} 256 : i32 loc(#loc1)
    %c64_i32 = arith.constant {async_task_id = dense<[0, 1]> : vector<2xi32>} 64 : i32 loc(#loc1)
    %c8_i32 = arith.constant {async_task_id = dense<[0, 1]> : vector<2xi32>} 8 : i32 loc(#loc1)
    %c63_i32 = arith.constant {async_task_id = dense<[0, 1]> : vector<2xi32>} 63 : i32 loc(#loc1)
    %c4_i32 = arith.constant 4 : i32 loc(#loc1)
    %c2_i32 = arith.constant 2 : i32 loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    tt.print " ugh " {hex = false, isSigned = array<i32 : 1>} : %c0_i32 : i32
    %0 = triton_gpu.local_alloc  : () -> !tt.memdesc<3x64x64xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %1 = triton_gpu.local_alloc  : () -> !tt.memdesc<3x64x256xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %2 = triton_gpu.local_alloc  : () -> !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %3 = triton_gpu.memdesc_subview %2[%c0_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %3, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %4 = triton_gpu.memdesc_subview %2[%c1_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %4, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %5 = triton_gpu.memdesc_subview %2[%c2_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %5, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %6 = triton_gpu.local_alloc  : () -> !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %7 = triton_gpu.local_alloc  : () -> !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %8 = triton_gpu.memdesc_subview %6[%c0_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %8, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %9 = triton_gpu.memdesc_subview %7[%c0_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %9, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %10 = triton_gpu.memdesc_subview %6[%c1_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %10, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %11 = triton_gpu.memdesc_subview %7[%c1_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %11, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %12 = triton_gpu.memdesc_subview %6[%c2_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %12, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %13 = triton_gpu.memdesc_subview %7[%c2_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %13, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %14 = triton_nvidia_gpu.get_canonical_warp_id : i32 loc(#loc)
    %15 = arith.divui %14, %c4_i32 : i32 loc(#loc)
    %16 = arith.cmpi uge, %15, %c0_i32 : i32 loc(#loc)
    %17 = arith.cmpi ult, %15, %c1_i32 : i32 loc(#loc)
    %18 = arith.andi %16, %17 : i1 loc(#loc)
    tt.print " wgid: " { hex = false, isSigned = array<i32: 1> } : %15 : i32
    scf.if %18 {
      triton_nvidia_gpu.reg_dealloc 40 loc(#loc)
      triton_nvidia_gpu.reg_dealloc 40 loc(#loc)
      %22 = arith.addi %arg3, %c63_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc30)
      %23 = arith.divsi %22, %c64_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc31)
      %24 = arith.addi %arg4, %c255_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc32)
      %25 = arith.divsi %24, %c256_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc33)
      %26 = arith.muli %23, %25 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc6)
      %27 = tt.get_program_id x {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc7)
      %28 = tt.get_num_programs x {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc8)
      %29 = arith.muli %25, %c8_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc9)
      %30 = arith.addi %arg5, %c63_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc34)
      %31 = arith.divsi %30, %c64_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc35)
      %32 = scf.for %arg9 = %27 to %26 step %28 iter_args(%arg10 = %c0_i32) -> (i32)  : i32 {
        %33 = arith.divsi %arg9, %29 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc12)
        %34 = arith.muli %33, %c8_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc13)
        %35 = arith.subi %23, %34 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc14)
        %36 = arith.minsi %35, %c8_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc15)
        %37 = arith.remsi %arg9, %29 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc16)
        %38 = arith.remsi %37, %36 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc17)
        %39 = arith.addi %34, %38 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc18)
        %40 = arith.divsi %37, %36 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc19)
        %41 = arith.muli %39, %c64_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc20)
        %42 = arith.muli %40, %c256_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc21)
        %43 = arith.muli %arg10, %31 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
        %44 = arith.divui %43, %c3_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
        %45 = arith.muli %44, %c3_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
        %46 = arith.subi %43, %45 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
        %47 = arith.andi %44, %c1_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
        %48 = arith.trunci %47 {async_task_id = dense<0> : vector<1xi32>} : i32 to i1 loc(#loc22)
        %49:3 = scf.for %arg11 = %c0_i32 to %31 step %c1_i32 iter_args(%arg12 = %c0_i32, %arg13 = %48, %arg14 = %46) -> (i32, i1, i32)  : i32 {
          %51 = triton_gpu.memdesc_subview %7[%arg14] {async_task_id = dense<0> : vector<1xi32>} : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc23)
          %52 = arith.xori %arg13, %true : i1 loc(#loc23)
          %53 = arith.extui %52 : i1 to i32 loc(#loc23)
          triton_nvidia_gpu.wait_barrier %51, %53 {async_task_id = dense<0> : vector<1xi32>} : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc23)
          %54 = triton_gpu.memdesc_subview %2[%arg14] {async_task_id = dense<0> : vector<1xi32>} : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
          triton_nvidia_gpu.barrier_expect %54, 40960 {async_task_id = dense<0> : vector<1xi32>}, %true : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
          %55 = triton_gpu.memdesc_subview %0[%arg14, %c0_i32, %c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !tt.memdesc<3x64x64xf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<64x64xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
          triton_nvidia_gpu.async_tma_copy_global_to_local %arg0[%41, %arg12] %55, %54, %true {async_task_id = dense<0> : vector<1xi32>} : <i8, 0>, <1xi64, #shared1, #triton_gpu.shared_memory, mutable> -> <64x64xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
          %56 = triton_gpu.memdesc_subview %1[%arg14, %c0_i32, %c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !tt.memdesc<3x64x256xf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<64x256xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
          triton_nvidia_gpu.async_tma_copy_global_to_local %arg1[%arg12, %42] %56, %54, %true {async_task_id = dense<0> : vector<1xi32>} : <i8, 0>, <1xi64, #shared1, #triton_gpu.shared_memory, mutable> -> <64x256xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
          %57 = arith.addi %arg12, %c64_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc24)
          %58 = arith.addi %arg14, %c1_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
          %59 = arith.cmpi uge, %58, %c3_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
          %60 = arith.cmpi ult, %58, %c3_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
          %61 = arith.addi %arg14, %c-2_i32 : i32 loc(#loc22)
          %62 = arith.select %59, %61, %58 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
          %63 = arith.xori %arg13, %true {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc22)
          %64 = arith.andi %59, %63 {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc22)
          %65 = arith.andi %60, %arg13 {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc22)
          %66 = arith.ori %64, %65 {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc22)
          scf.yield {async_task_id = dense<0> : vector<1xi32>} %57, %66, %62 : i32, i1, i32 loc(#loc25)
        } {async_task_id = dense<0> : vector<1xi32>} loc(#loc22)
        %50 = arith.addi %arg10, %c1_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc11)
        scf.yield %50 : i32 loc(#loc11)
      } {async_task_id = dense<0> : vector<1xi32>} loc(#loc11)
    } {async_task_id = dense<0> : vector<1xi32>} loc(#loc)
    %19 = arith.cmpi uge, %15, %c1_i32 : i32 loc(#loc)
    %20 = arith.cmpi ult, %15, %c2_i32 : i32 loc(#loc)
    %21 = arith.andi %19, %20 : i1 loc(#loc)
    scf.if %21 {
      triton_nvidia_gpu.reg_alloc 232 loc(#loc)
      triton_nvidia_gpu.reg_alloc 232 loc(#loc)
      %22 = arith.addi %arg3, %c63_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc30)
      %23 = arith.divsi %22, %c64_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc31)
      %24 = arith.addi %arg4, %c255_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc32)
      %25 = arith.divsi %24, %c256_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc33)
      %26 = arith.muli %23, %25 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc6)
      %27 = tt.get_program_id x {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc7)
      %28 = tt.get_num_programs x {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc8)
      %29 = arith.muli %25, %c8_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc9)
      %30 = arith.addi %arg5, %c63_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc34)
      %31 = arith.divsi %30, %c64_i32 {async_task_id = dense<[0, 1]> : vector<2xi32>} : i32 loc(#loc35)
      %32 = scf.for %arg9 = %27 to %26 step %28 iter_args(%arg10 = %c0_i32) -> (i32)  : i32 {
        tt.print " inner: " {hex = false, isSigned = array<i32: 1>} : %arg9 : i32
        %33 = arith.divsi %arg9, %29 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc12)
        %34 = arith.muli %33, %c8_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc13)
        %35 = arith.subi %23, %34 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc14)
        %36 = arith.minsi %35, %c8_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc15)
        %37 = arith.remsi %arg9, %29 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc16)
        %38 = arith.remsi %37, %36 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc17)
        %39 = arith.addi %34, %38 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc18)
        %40 = arith.divsi %37, %36 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc19)
        %41 = arith.muli %39, %c64_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc20)
        %42 = arith.muli %40, %c256_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc21)
        %43 = arith.muli %arg10, %31 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
        %44 = arith.divui %43, %c3_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
        %45 = arith.muli %44, %c3_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
        %46 = arith.subi %43, %45 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
        %47 = arith.andi %44, %c1_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
        %48 = arith.trunci %47 {async_task_id = dense<1> : vector<1xi32>} : i32 to i1 loc(#loc22)
        %49:3 = scf.for %arg11 = %c0_i32 to %31 step %c1_i32 iter_args(%arg12 = %cst, %arg13 = %48, %arg14 = %46) -> (tensor<64x256xf32, #mma>, i1, i32)  : i32 {
          tt.print " inner: " {hex = false, isSigned = array<i32: 1>} : %arg11 : i32
          %53 = triton_gpu.memdesc_subview %2[%arg14] {async_task_id = dense<1> : vector<1xi32>} : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
          %54 = arith.extsi %arg13 {async_task_id = dense<1> : vector<1xi32>} : i1 to i32 loc(#loc)
          triton_nvidia_gpu.wait_barrier %53, %54 {async_task_id = dense<1> : vector<1xi32>} : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
          %55 = triton_gpu.memdesc_subview %0[%arg14, %c0_i32, %c0_i32] {async_task_id = dense<1> : vector<1xi32>} : !tt.memdesc<3x64x64xf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<64x64xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
          %56 = triton_gpu.memdesc_subview %1[%arg14, %c0_i32, %c0_i32] {async_task_id = dense<1> : vector<1xi32>} : !tt.memdesc<3x64x256xf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<64x256xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
          %57 = triton_nvidia_gpu.warp_group_dot %55, %56, %arg12 {async_task_id = dense<1> : vector<1xi32>, inputPrecision = 0 : i32} : !tt.memdesc<64x64xf16, #shared, #triton_gpu.shared_memory, mutable> * !tt.memdesc<64x256xf16, #shared, #triton_gpu.shared_memory, mutable> -> tensor<64x256xf32, #mma> loc(#loc26)
          %58 = triton_gpu.memdesc_subview %7[%arg14] {async_task_id = dense<1> : vector<1xi32>} : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc26)
          %thread_id_x = gpu.thread_id  x loc(#loc26)
          %59 = builtin.unrealized_conversion_cast %thread_id_x : index to i32 loc(#loc26)
          %60 = arith.remui %59, %c128_i32 : i32 loc(#loc26)
          %61 = arith.divui %60, %c8_i32 : i32 loc(#loc26)
          %62 = arith.divui %61, %c4_i32 : i32 loc(#loc26)
          %63 = arith.remui %61, %c4_i32 : i32 loc(#loc26)
          %64 = arith.xori %63, %62 : i32 loc(#loc26)
          %65 = arith.muli %64, %c4_i32 : i32 loc(#loc26)
          %66 = arith.addi %65, %63 : i32 loc(#loc26)
          %67 = arith.remui %60, %c8_i32 : i32 loc(#loc26)
          %68 = arith.cmpi eq, %67, %c0_i32 : i32 loc(#loc26)
          %69 = arith.cmpi ult, %66, %c1_i32 : i32 loc(#loc26)
          %70 = arith.andi %68, %69 : i1 loc(#loc26)
          triton_nvidia_gpu.mbarrier_arrive %58, %70, %66 {async_task_id = dense<1> : vector<1xi32>, operandSegmentSizes = array<i32: 1, 1, 1>, trackAsyncOp = false} : !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable>, i1, i32 loc(#loc26)
          %71 = arith.addi %arg14, %c1_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
          %72 = arith.cmpi uge, %71, %c3_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
          %73 = arith.cmpi ult, %71, %c3_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
          %74 = arith.addi %arg14, %c-2_i32 : i32 loc(#loc22)
          %75 = arith.select %72, %74, %71 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
          %76 = arith.xori %arg13, %true {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc22)
          %77 = arith.andi %72, %76 {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc22)
          %78 = arith.andi %73, %arg13 {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc22)
          %79 = arith.ori %77, %78 {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc22)
          scf.yield {async_task_id = dense<1> : vector<1xi32>} %57, %79, %75 : tensor<64x256xf32, #mma>, i1, i32 loc(#loc25)
        } {async_task_id = dense<1> : vector<1xi32>} loc(#loc22)
        %50 = arith.truncf %49#0 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x256xf32, #mma> to tensor<64x256xf16, #mma> loc(#loc27)
        %51 = triton_gpu.local_alloc %50 {async_task_id = dense<1> : vector<1xi32>} : (tensor<64x256xf16, #mma>) -> !tt.memdesc<64x256xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc28)
        triton_nvidia_gpu.fence_async_shared {async_task_id = dense<1> : vector<1xi32>, bCluster = false} loc(#loc28)
        triton_nvidia_gpu.async_tma_copy_local_to_global %arg2[%41, %42] %51 {async_task_id = dense<1> : vector<1xi32>} : <i8, 0>, <64x256xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc28)
        triton_nvidia_gpu.async_tma_store_wait {async_task_id = dense<1> : vector<1xi32>, pendings = 0 : i32} loc(#loc28)
        %52 = arith.addi %arg10, %c1_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc11)
        scf.yield %52 : i32 loc(#loc11)
      } {async_task_id = dense<1> : vector<1xi32>} loc(#loc11)
    } {async_task_id = dense<1> : vector<1xi32>} loc(#loc)
    tt.return loc(#loc29)
  } loc(#loc)
} loc(#loc)
#loc1 = loc(unknown)
#loc2 = loc("/home/bertrand/osmeta-triton/triton/python/triton/language/standard.py":40:22)
#loc3 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":26:27)
#loc4 = loc("/home/bertrand/osmeta-triton/triton/python/triton/language/standard.py":40:28)
#loc5 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":26:54)
#loc6 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":26:43)
#loc7 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":27:35)
#loc8 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":27:66)
#loc9 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":31:42)
#loc10 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":60:37)
#loc11 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":27:50)
#loc12 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":32:26)
#loc13 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":33:33)
#loc14 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":34:39)
#loc15 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":34:52)
#loc16 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":35:38)
#loc17 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":35:58)
#loc18 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":35:31)
#loc19 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":36:44)
#loc20 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":50:26)
#loc21 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":51:26)
#loc22 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":60:26)
#loc23 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":64:109)
#loc24 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":69:22)
#loc25 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":69:12)
#loc26 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":67:39)
#loc27 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":70:27)
#loc28 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":75:56)
#loc29 = loc("/home/bertrand/osmeta-triton/triton/python/tutorials/impls/persistent_tma_ws.py":27:4)
#loc30 = loc(callsite(#loc2 at #loc3))
#loc31 = loc(callsite(#loc4 at #loc3))
#loc32 = loc(callsite(#loc2 at #loc5))
#loc33 = loc(callsite(#loc4 at #loc5))
#loc34 = loc(callsite(#loc2 at #loc10))
#loc35 = loc(callsite(#loc4 at #loc10))
